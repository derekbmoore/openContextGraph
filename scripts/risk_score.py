#!/usr/bin/env python3
"""
PR Risk Scoring Script

Quantifies change risk before merge based on:
- Lines changed (size heuristic)
- File sensitivity (auth, database, infra weighted higher)
- AI-generated code flag
- Historical defect rate (placeholder)

NIST AI RMF: GOVERN 1.1 (Policies), MANAGE 4.1 (Incident Prevention)

Usage:
    python scripts/risk_score.py [--pr-number PR_NUM]
    python scripts/risk_score.py --diff-only
"""

import argparse
import subprocess
import sys
from dataclasses import dataclass
from pathlib import Path


@dataclass
class RiskResult:
    score: int  # 0-100
    level: str  # LOW, MEDIUM, HIGH, CRITICAL
    details: list[str]


# File sensitivity weights
SENSITIVITY_WEIGHTS = {
    # CRITICAL - Security and infrastructure
    "backend/api/middleware/auth.py": 50,
    "backend/core/context.py": 40,
    ".github/workflows/": 40,
    "infra/": 35,
    ".env": 50,
    
    # HIGH - Data layer
    "backend/memory/": 30,
    "backend/etl/": 25,
    "backend/db/": 30,
    
    # MEDIUM - Agent code
    "backend/agents/": 20,
    
    # LOW - Frontend and docs
    "frontend/": 5,
    "docs/": 2,
    
    # DEFAULT
    "*": 10,
}

# Keywords indicating AI-generated code
AI_MARKERS = [
    "generated by",
    "auto-generated",
    "copilot",
    "claude",
    "gpt-4",
    "agent_id",
]


def get_changed_files(pr_number: int = None) -> list[tuple[str, int, int]]:
    """
    Get list of changed files with lines added/removed.
    
    Returns: [(filename, lines_added, lines_removed), ...]
    """
    if pr_number:
        # Get diff from GitHub PR
        cmd = ["gh", "pr", "diff", str(pr_number), "--name-only"]
        result = subprocess.run(cmd, capture_output=True, text=True)
        files = result.stdout.strip().split("\n")
        
        # Get stat for each file
        changes = []
        for f in files:
            if f:
                changes.append((f, 10, 5))  # Placeholder stats
        return changes
    else:
        # Get diff from staged changes
        cmd = ["git", "diff", "--cached", "--numstat"]
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        changes = []
        for line in result.stdout.strip().split("\n"):
            if line:
                parts = line.split("\t")
                if len(parts) == 3:
                    added = int(parts[0]) if parts[0] != "-" else 0
                    removed = int(parts[1]) if parts[1] != "-" else 0
                    filename = parts[2]
                    changes.append((filename, added, removed))
        
        return changes


def get_file_sensitivity(filepath: str) -> int:
    """Get sensitivity weight for a file path."""
    for pattern, weight in SENSITIVITY_WEIGHTS.items():
        if pattern in filepath or filepath.startswith(pattern.rstrip("/")):
            return weight
    return SENSITIVITY_WEIGHTS.get("*", 10)


def check_ai_generated(filepath: str) -> bool:
    """Check if file contains AI-generation markers."""
    try:
        with open(filepath, "r") as f:
            content = f.read().lower()
            return any(marker in content for marker in AI_MARKERS)
    except Exception:
        return False


def calculate_risk(changes: list[tuple[str, int, int]]) -> RiskResult:
    """
    Calculate overall PR risk score.
    
    Scoring factors:
    - Base: Lines changed (1 point per 10 lines)
    - Sensitivity: File weight multiplier
    - AI flag: +15 points if AI-generated code detected
    - Large change: +10 if >500 lines
    """
    details = []
    total_score = 0
    total_lines = 0
    max_sensitivity = 0
    ai_detected = False
    
    for filepath, added, removed in changes:
        lines = added + removed
        total_lines += lines
        
        # File sensitivity
        sensitivity = get_file_sensitivity(filepath)
        max_sensitivity = max(max_sensitivity, sensitivity)
        
        # Line-based score
        file_score = (lines / 10) * (sensitivity / 10)
        total_score += file_score
        
        # Check for AI markers
        if Path(filepath).exists() and check_ai_generated(filepath):
            ai_detected = True
            details.append(f"‚ö†Ô∏è  AI-generated code detected: {filepath}")
        
        if sensitivity >= 30:
            details.append(f"üîí High-sensitivity file: {filepath} (weight: {sensitivity})")
    
    # Large change penalty
    if total_lines > 500:
        total_score += 10
        details.append(f"üìä Large change: {total_lines} lines")
    
    # AI detection penalty
    if ai_detected:
        total_score += 15
    
    # Normalize to 0-100
    score = min(100, int(total_score))
    
    # Determine level
    if score >= 70:
        level = "CRITICAL"
    elif score >= 40:
        level = "HIGH"
    elif score >= 20:
        level = "MEDIUM"
    else:
        level = "LOW"
    
    details.insert(0, f"Total lines changed: {total_lines}")
    details.insert(1, f"Max file sensitivity: {max_sensitivity}")
    
    return RiskResult(score=score, level=level, details=details)


def print_result(result: RiskResult):
    """Print risk assessment result."""
    # Color codes
    colors = {
        "LOW": "\033[92m",      # Green
        "MEDIUM": "\033[93m",   # Yellow
        "HIGH": "\033[91m",     # Red
        "CRITICAL": "\033[95m", # Magenta
    }
    reset = "\033[0m"
    
    color = colors.get(result.level, "")
    
    print("\n" + "=" * 60)
    print("PR RISK ASSESSMENT")
    print("=" * 60)
    print(f"\nRisk Score: {color}{result.score}/100{reset}")
    print(f"Risk Level: {color}{result.level}{reset}")
    print("\nDetails:")
    for detail in result.details:
        print(f"  ‚Ä¢ {detail}")
    print("\n" + "=" * 60)
    
    # Recommendations
    if result.level == "CRITICAL":
        print("\nüö® CRITICAL: Requires senior engineer review")
        print("   Consider breaking into smaller PRs")
    elif result.level == "HIGH":
        print("\n‚ö†Ô∏è  HIGH: Requires careful review")
        print("   Ensure tests cover security-sensitive changes")
    elif result.level == "MEDIUM":
        print("\nüìã MEDIUM: Standard review process")
    else:
        print("\n‚úÖ LOW: Routine change")


def main():
    parser = argparse.ArgumentParser(description="Calculate PR risk score")
    parser.add_argument("--pr-number", type=int, help="GitHub PR number")
    parser.add_argument("--diff-only", action="store_true", help="Use staged diff only")
    parser.add_argument("--json", action="store_true", help="Output as JSON")
    args = parser.parse_args()
    
    # Get changed files
    changes = get_changed_files(args.pr_number if not args.diff_only else None)
    
    if not changes:
        print("No changes detected.")
        sys.exit(0)
    
    # Calculate risk
    result = calculate_risk(changes)
    
    if args.json:
        import json
        print(json.dumps({
            "score": result.score,
            "level": result.level,
            "details": result.details,
        }, indent=2))
    else:
        print_result(result)
    
    # Exit with non-zero for CRITICAL
    if result.level == "CRITICAL":
        sys.exit(1)


if __name__ == "__main__":
    main()
